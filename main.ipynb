{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import random\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "from utils import net_builder, get_logger, count_parameters\n",
    "from train_utils import TBLog, get_SGD, get_cosine_schedule_with_warmup\n",
    "from models.fixmatch.fixmatch import FixMatch\n",
    "from datasets.ssl_dataset import SSL_Dataset\n",
    "from datasets.data_utils import get_data_loader\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "from scipy.special import softmax\n",
    "from scipy.optimize import minimize\n",
    "from pycalib.metrics import classwise_ECE, conf_ECE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'save_dir': './saved_models',\n",
    "    'resume': 'store_true',\n",
    "    'save_name': 'fixmatch',\n",
    "    'load_path': None,\n",
    "    'overwrite': 'store_true',\n",
    "    'epoch': 300,\n",
    "    'num_train_iter': 2**20,\n",
    "    'num_eval_iter': 10000,\n",
    "    'num_labels': 4000,\n",
    "    'batch_size': 64,\n",
    "    'uratio': 7,\n",
    "    'eval_batch_size': 1024,\n",
    "    'hard_label': True,\n",
    "    'T': 0.5,\n",
    "    'p_cutoff': 0.95,\n",
    "    'ema_m': 0.999,\n",
    "    'ulb_loss_ratio': 1.0,\n",
    "    'lr': 0.03,\n",
    "    'momentum': 0.9,\n",
    "    'weight_decay': 5e-4,\n",
    "    'net': 'WideResNet',\n",
    "    'net_from_name': False,\n",
    "    'depth': 28,\n",
    "    'widen_factor': 2,\n",
    "    'leaky_slope': 0.1,\n",
    "    'dropout': 0.0,\n",
    "    'data_dir': './data',\n",
    "    'dataset': 'cifar10',\n",
    "    'train_sampler': 'RandomSampler',\n",
    "    'num_classes': 10,\n",
    "    'amp': 'store_true',\n",
    "    'gpu': 0,\n",
    "    'multiprocessing_distributed': 'store_true',\n",
    "    'rank': 0\n",
    "}\n",
    "\n",
    "from argparse import Namespace\n",
    "args = Namespace(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join(args.save_dir, args.save_name)\n",
    "\n",
    "global best_acc1\n",
    "\n",
    "seed = 1\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth in <models.nets.wrn.build_WideResNet object at 0x7ff63b72e580> is overlapped by kwargs: 28 -> 28\n",
      "widen_factor in <models.nets.wrn.build_WideResNet object at 0x7ff63b72e580> is overlapped by kwargs: 2 -> 2\n",
      "leaky_slope in <models.nets.wrn.build_WideResNet object at 0x7ff63b72e580> is overlapped by kwargs: 0.0 -> 0.1\n",
      "bn_momentum in <models.nets.wrn.build_WideResNet object at 0x7ff63b72e580> is overlapped by kwargs: 0.01 -> 0.0010000000000000009\n",
      "dropRate in <models.nets.wrn.build_WideResNet object at 0x7ff63b72e580> is overlapped by kwargs: 0.0 -> 0.0\n"
     ]
    }
   ],
   "source": [
    "args.bn_momentum = 1.0 - args.ema_m\n",
    "_net_builder = net_builder(args.net, \n",
    "                            args.net_from_name,\n",
    "                            {'depth': args.depth, \n",
    "                            'widen_factor': args.widen_factor,\n",
    "                            'leaky_slope': args.leaky_slope,\n",
    "                            'bn_momentum': args.bn_momentum,\n",
    "                            'dropRate': args.dropout})\n",
    "\n",
    "model = FixMatch(_net_builder,\n",
    "                    args.num_classes,\n",
    "                    args.ema_m,\n",
    "                    args.T,\n",
    "                    args.p_cutoff,\n",
    "                    args.ulb_loss_ratio,\n",
    "                    args.hard_label,\n",
    "                    num_eval_iter=args.num_eval_iter)\n",
    "\n",
    "\n",
    "optimizer = get_SGD(model.train_model, 'SGD', args.lr, args.momentum, args.weight_decay)\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer,\n",
    "                                            args.num_train_iter,\n",
    "                                            num_warmup_steps=args.num_train_iter*0)\n",
    "\n",
    "model.set_optimizer(optimizer, scheduler)\n",
    "model.train_model = model.train_model.cuda()\n",
    "model.eval_model = model.eval_model.cuda()\n",
    "\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "d\n",
      "a\n",
      "d\n",
      "a\n",
      "[!] data loader keys: dict_keys(['train_lb', 'train_ulb', 'eval'])\n"
     ]
    }
   ],
   "source": [
    "train_dset = SSL_Dataset(name=args.dataset, train=True, \n",
    "                            num_classes=args.num_classes, data_dir=args.data_dir)\n",
    "lb_dset, ulb_dset, calib_dset = train_dset.get_ssl_dset(args.num_labels)\n",
    "\n",
    "_eval_dset = SSL_Dataset(name=args.dataset, train=False, \n",
    "                            num_classes=args.num_classes, data_dir=args.data_dir)\n",
    "eval_dset = _eval_dset.get_dset()\n",
    "\n",
    "loader_dict = {}\n",
    "dset_dict = {'train_lb': lb_dset, 'train_ulb': ulb_dset, 'eval': eval_dset}\n",
    "\n",
    "loader_dict['train_lb'] = get_data_loader(dset_dict['train_lb'],\n",
    "                                            args.batch_size,\n",
    "                                            data_sampler = args.train_sampler,\n",
    "                                            num_epochs=args.epoch)\n",
    "\n",
    "loader_dict['train_ulb'] = get_data_loader(dset_dict['train_ulb'],\n",
    "                                            args.batch_size*args.uratio,\n",
    "                                            data_sampler = args.train_sampler,\n",
    "                                            num_epochs=args.epoch)\n",
    "\n",
    "loader_dict['eval'] = get_data_loader(dset_dict['eval'],\n",
    "                                        args.eval_batch_size)\n",
    "\n",
    "model.set_data_loader(loader_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1/1\n",
      "0 iteration, USE_EMA: True, {'train/sup_loss': tensor(2.2393, device='cuda:0'), 'train/unsup_loss': tensor(0., device='cuda:0'), 'train/total_loss': tensor(2.2393, device='cuda:0'), 'train/mask_ratio': tensor(1., device='cuda:0'), 'lr': 0.029999999999355702, 'train/prefecth_time': 0.7456192016601563, 'train/run_time': 0.2748617858886719, 'eval/loss': tensor(8.2592, device='cuda:0'), 'eval/top-1-acc': tensor(0.1000, device='cuda:0')}, BEST_EVAL_ACC: 0.09999999403953552, at 0 iters\n",
      "model saved: ./saved_models/fixmatch/model_best.pth\n",
      "Finished epoch 1/1\n",
      "model saved: ./saved_models/fixmatch/latest_model.pth\n"
     ]
    }
   ],
   "source": [
    "trainer = model.train\n",
    "for epoch in range(args.epoch):\n",
    "    print(f'Starting epoch {epoch + 1}/{args.epoch}')\n",
    "    trainer(args)\n",
    "    print(f'Finished epoch {epoch + 1}/{args.epoch}')\n",
    "\n",
    "model.save_model('latest_model.pth', save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth in <models.nets.wrn.build_WideResNet object at 0x7ff62557bdf0> is overlapped by kwargs: 28 -> 28\n",
      "widen_factor in <models.nets.wrn.build_WideResNet object at 0x7ff62557bdf0> is overlapped by kwargs: 2 -> 2\n",
      "leaky_slope in <models.nets.wrn.build_WideResNet object at 0x7ff62557bdf0> is overlapped by kwargs: 0.0 -> 0.1\n",
      "dropRate in <models.nets.wrn.build_WideResNet object at 0x7ff62557bdf0> is overlapped by kwargs: 0.0 -> 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17620000000000002\n",
      "0.14690000000000003\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = os.path.join('./saved_models/fixmatch/model_best.pth')\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "load_model = checkpoint['train_model']\n",
    "\n",
    "_net_builder = net_builder(args.net, \n",
    "                            args.net_from_name,\n",
    "                            {'depth': args.depth, \n",
    "                            'widen_factor': args.widen_factor,\n",
    "                            'leaky_slope': args.leaky_slope,\n",
    "                            'dropRate': args.dropout})\n",
    "\n",
    "net = _net_builder(num_classes=args.num_classes)\n",
    "\n",
    "net.load_state_dict(load_model)\n",
    "if torch.cuda.is_available():\n",
    "    net.cuda()\n",
    "net.eval()\n",
    "\n",
    "calib_loader = get_data_loader(calib_dset,\n",
    "                                args.eval_batch_size, \n",
    "                                num_workers=1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = np.empty((0, 10))\n",
    "    encoding = []\n",
    "    for image, target in calib_loader:\n",
    "        image = image.type(torch.FloatTensor).cuda()\n",
    "        logits = np.append(logits, net(image).cpu().detach().numpy(), axis=0)\n",
    "        for value in target:\n",
    "            encoding.append(np.eye(10)[value])\n",
    "    \n",
    "    def vector_scale_loss(x, *args):\n",
    "        sm = softmax(np.multiply(logits, x[0:-1]) + x[-1], axis=1)\n",
    "        return log_loss(encoding, sm)\n",
    "\n",
    "    min_obj = minimize(vector_scale_loss, [1 for i in range(11)], method='Nelder-Mead')\n",
    "\n",
    "    uncal_scores = softmax(logits, axis=1)\n",
    "    vector_scores = softmax(np.multiply(logits, min_obj.x[0:-1]) + min_obj.x[-1], axis=1)\n",
    "\n",
    "    conf_ece = conf_ECE(uncal_scores, encoding)\n",
    "    print(conf_ece)\n",
    "\n",
    "    conf_ece = conf_ECE(vector_scores, encoding)\n",
    "    print(conf_ece)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       message: Optimization terminated successfully.\n",
       "       success: True\n",
       "        status: 0\n",
       "           fun: 0.47924357241256127\n",
       "             x: [ 5.179e-01  4.114e-01  5.672e-01  5.684e-01  5.539e-01\n",
       "                  9.161e-01  7.049e-01  6.259e-01  4.441e-01  1.062e+00\n",
       "                  2.171e+00]\n",
       "           nit: 760\n",
       "          nfev: 1076\n",
       " final_simplex: (array([[ 5.179e-01,  4.114e-01, ...,  1.062e+00,\n",
       "                         2.171e+00],\n",
       "                       [ 5.179e-01,  4.113e-01, ...,  1.062e+00,\n",
       "                         2.171e+00],\n",
       "                       ...,\n",
       "                       [ 5.180e-01,  4.113e-01, ...,  1.062e+00,\n",
       "                         2.171e+00],\n",
       "                       [ 5.179e-01,  4.113e-01, ...,  1.062e+00,\n",
       "                         2.171e+00]]), array([ 4.792e-01,  4.792e-01,  4.792e-01,  4.792e-01,\n",
       "                        4.792e-01,  4.792e-01,  4.792e-01,  4.792e-01,\n",
       "                        4.792e-01,  4.792e-01,  4.792e-01,  4.792e-01]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_obj"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
